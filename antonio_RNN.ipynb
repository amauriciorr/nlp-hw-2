{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "970AS3pGerFA"
   },
   "source": [
    "# DS-GA 1011 Homework 2\n",
    "## N-Gram and Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jDFAcX6MerFB",
    "outputId": "e3408ecc-e255-43b9-80dd-cc8faa32fc8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "#import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpLkEDTmerFE"
   },
   "source": [
    "## I. N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UYZDUMjerFF"
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mx4F7xByerFF"
   },
   "outputs": [],
   "source": [
    "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
    "    \n",
    "    datasets = json.load(open(filename, 'r'))\n",
    "    for name in datasets:\n",
    "        datasets[name] = [x.split() for x in datasets[name]]\n",
    "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return datasets, vocab\n",
    "\n",
    "def perplexity(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        logp_total += model.sequence_logp(sequence)\n",
    "        n_total += len(sequence) + 1  \n",
    "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVfI-rjyerFH"
   },
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHRoN6lMerFI"
   },
   "outputs": [],
   "source": [
    "class NGramAdditive(object):\n",
    "    def __init__(self, n, delta, vsize):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "        self.vsize = vsize\n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "            for i in range(len(padded_sequence) - self.n+1):\n",
    "                ngram = tuple(padded_sequence[i:i+self.n])\n",
    "                prefix, word = ngram[:-1], ngram[-1]\n",
    "                self.count[prefix][word] += 1\n",
    "                self.total[prefix] += 1\n",
    "                \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_prob(ngram))\n",
    "        return total_logp\n",
    "\n",
    "    def ngram_prob(self, ngram):\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        prob = ((self.delta + self.count[prefix][word]) / \n",
    "                (self.total[prefix] + self.delta*self.vsize))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlKfaPdBerFK",
    "outputId": "247f5bc5-825c-4728-9c86-28612b231c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33175\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
     ]
    }
   ],
   "source": [
    "wiki_datasets, vocab = load_wikitext()\n",
    "\n",
    "delta = 0.0005\n",
    "for n in [2, 3, 4]:\n",
    "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "    lm.estimate(wiki_datasets['train'])\n",
    "\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, wiki_datasets['train'])))\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, wiki_datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T50510neerFN"
   },
   "source": [
    "### I.1 Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UN9rc5BperFO"
   },
   "outputs": [],
   "source": [
    "class NGramInterpolation(object):\n",
    "    def __init__(self, n, lambdas, vsize, total_word_count):\n",
    "        self.n = n\n",
    "        self.lambdas = lambdas\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "        self.vsize = vsize\n",
    "        self.total_word_count = total_word_count\n",
    "        \n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "            for i in range(self.n-1, len(padded_sequence)):\n",
    "                ngram = tuple(padded_sequence[i-self.n+1:i+1])\n",
    "                for j in range(1, self.n):\n",
    "                    prefix, word = ngram[:j], ngram[j]\n",
    "                    self.count[prefix][word] += 1\n",
    "                    self.total[prefix] += 1 \n",
    "                    self.total[word] += 1\n",
    "                    \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_prob(ngram))\n",
    "        return total_logp\n",
    "\n",
    "    def ngram_prob(self, ngram):\n",
    "        prob = self.lambdas[-1]/self.vsize\n",
    "        word = ngram[-1]\n",
    "        prob += (self.total[word] / self.total_word_count) * self.lambdas[-2]\n",
    "        for nk in range(len(ngram) - 1):\n",
    "            prefix = ngram[nk:-1]\n",
    "            if self.total[prefix] > 1e-9:\n",
    "                prob += ((self.count[prefix][word])/ (self.total[prefix])) * self.lambdas[nk]\n",
    "            \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "or9C_sfGerFQ",
    "outputId": "6554e078-73ae-4a26-84d2-3abc03a80088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924754\n"
     ]
    }
   ],
   "source": [
    "total_wc = len([t for ts in wiki_datasets['train'] for t in ts])\n",
    "print(total_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Te3XJUyverFT"
   },
   "source": [
    "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNaIztgJerFU",
    "outputId": "5e117ef4-b6d3-4311-cbc7-128a1d3a18d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas add to: 1.0\n",
      "lambdas = [0.20953827 0.79046173]\n",
      "Interpolation, n=2 | Train Perplexity: 262.011\n",
      "Interpolation, n=2 | Valid Perplexity: 516.755\n",
      "Lambdas add to: 0.9999999999999999\n",
      "lambdas = [0.25564492 0.49439094 0.24996414]\n",
      "Interpolation, n=3 | Train Perplexity: 18.841\n",
      "Interpolation, n=3 | Valid Perplexity: 170.144\n",
      "Lambdas add to: 1.0\n",
      "lambdas = [0.13951532 0.01370735 0.57312293 0.27365439]\n",
      "Interpolation, n=4 | Train Perplexity: 10.124\n",
      "Interpolation, n=4 | Valid Perplexity: 158.523\n",
      "A good configuration has been found\n"
     ]
    }
   ],
   "source": [
    "found = False\n",
    "while not(found):\n",
    "    perplexs = []\n",
    "    for n in [2, 3, 4]:\n",
    "        #lambdas = [1/(n+1) for i in range(n+1)]\n",
    "\n",
    "        # Uncomment to input in the following format #.##,#.##,#.##(...)\n",
    "        # lam = input()\n",
    "        # lambdas = [float(i) for i in lam.split(\",\")]\n",
    "\n",
    "        # Random search using Dirichlet Distribution (Notice: lambdas add to 1)\n",
    "        lambdas = np.ravel(np.random.dirichlet(np.ones(n),size=1))\n",
    "        print('Lambdas add to:',sum(np.ravel(np.random.dirichlet(np.ones(n),size=1))))\n",
    "\n",
    "        lm = NGramInterpolation(n=n, lambdas=lambdas, vsize=len(vocab)+1, total_word_count = total_wc)  # +1 is for <eos>\n",
    "        lm.estimate(wiki_datasets['train'])\n",
    "\n",
    "        print(\"lambdas = \" + str(lambdas))\n",
    "        print(\"Interpolation, n=%d | Train Perplexity: %.3f\" % (n, perplexity(lm, wiki_datasets['train'])))\n",
    "        print(\"Interpolation, n=%d | Valid Perplexity: %.3f\" % (n, perplexity(lm, wiki_datasets['valid'])))\n",
    "        perplexs.append(perplexity(lm, wiki_datasets['valid']))\n",
    "        \n",
    "        # If initial perplexity is not better than Additive Smoothing\n",
    "        if (n == 2) and perplexs[n-2] > 525:\n",
    "            break\n",
    "        \n",
    "        # If the next n-gram is not better than last one found\n",
    "        if ( n > 2 ):\n",
    "            if (perplexs[n-2] < perplexs[n-3]):\n",
    "                if n == 4:\n",
    "                    found = True\n",
    "                    print(\"A good configuration has been found\")\n",
    "                continue\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJSFMgxlerFl"
   },
   "source": [
    "## II. Neural Language Modeling with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8adLmVCerFl"
   },
   "source": [
    "## LSTM RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Hegs1CIerFm"
   },
   "outputs": [],
   "source": [
    "# eventually need to cut this list down, don't think we use all these\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "import pickle as pkl\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "    path = 'gdrive/My Drive/'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "./\n"
     ]
    }
   ],
   "source": [
    "print(current_device)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl-TRA7CerFo"
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(document):\n",
    "    tokens = tokenizer(document)\n",
    "    tokens = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvQEdk2eerFq"
   },
   "outputs": [],
   "source": [
    "def load_wikitext(fname):\n",
    "    full_lines = []\n",
    "    datasets_text = {\n",
    "        'train': [],\n",
    "        'valid': [],\n",
    "        'test': [],\n",
    "    }\n",
    "    for jline in jsonlines.open(fname):\n",
    "        for split in datasets_text.keys():\n",
    "            for sentence in tqdm(jline[split]):\n",
    "                datasets_text[split].append(tokenize(sentence))\n",
    "        \n",
    "    return datasets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNFvKKjFerFs"
   },
   "outputs": [],
   "source": [
    "# wikitext = load_wikitext('/Users/Antonio/Downloads/wikitext2-sentencized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6T1E5F8serFu"
   },
   "outputs": [],
   "source": [
    "# pkl.dump(wikitext['train'], open('wiki_train_tokenized.p','wb'))\n",
    "# pkl.dump(wikitext['valid'], open('wiki_val_tokenized.p', 'wb'))\n",
    "# pkl.dump(wikitext['test'], open('wiki_test_tokenized.p','wb'))\n",
    "# pkl.dump(wikitext, open('wiki_all_tokenized.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAqLStSCerFz"
   },
   "outputs": [],
   "source": [
    "def load_wiki_data(path):\n",
    "    wiki_train = pkl.load(open(path+'wiki_train_tokenized.p','rb'))\n",
    "    wiki_val = pkl.load(open(path+'wiki_val_tokenized.p','rb'))\n",
    "    wiki_test = pkl.load(open(path+'wiki_test_tokenized.p','rb'))\n",
    "    wiki_all = pkl.load(open(path+'wiki_all_tokenized.p','rb'))\n",
    "    \n",
    "    return wiki_train, wiki_val, wiki_test, wiki_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fPjtGRFerF2"
   },
   "outputs": [],
   "source": [
    "wiki_train, wiki_val, wiki_test, wiki_all = load_wiki_data('gdrive/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkKdXYvserF4"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "\n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXVFQ55perF6",
    "outputId": "da71d33a-0f83-41d9-a52a-3fa17c995f9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [01:37<00:00, 800.05it/s] \n"
     ]
    }
   ],
   "source": [
    "# wiki_dict = Dictionary(wiki_all, include_valid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeV4I8C6erF8"
   },
   "outputs": [],
   "source": [
    "# pkl.dump(wiki_dict, open('wiki_dictionary.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rj2DtxWerF-"
   },
   "source": [
    "# Included pickled `wiki_dict` to avoid creating object\n",
    "takes around 1.5 mins to create.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKgrJeu0erF_"
   },
   "outputs": [],
   "source": [
    "wiki_dict = pkl.load(open('gdrive/My Drive/wiki_dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1iKv5cIerGA"
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "        \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sN671kCherGC"
   },
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 2\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiJAuRY5erGE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "\n",
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeSHZLLperGF"
   },
   "outputs": [],
   "source": [
    "def build_ngram_loader(dataset, dataset_dict, ngram_order, batch_size=2048):\n",
    "    tokenized_ngram_dataset = tokenize_dataset(dataset, dataset_dict, ngram_order)\n",
    "    #sliced_ngram = slice_sequences_given_order(tokenized_ngram_dataset, ngram_order)\n",
    "\n",
    "    ngram_datasets = {}\n",
    "    ngram_loaders = {}\n",
    "    for split, tokenized_dataset in tokenized_ngram_dataset.items():\n",
    "        ngram_datasets[split] = TensoredDataset(tokenized_dataset)\n",
    "    \n",
    "    for split, tensored_dataset in ngram_datasets.items():\n",
    "        ngram_loaders[split] = DataLoader(tensored_dataset, batch_size=batch_size, shuffle=True,\\\n",
    "                                          collate_fn=pad_collate_fn)\n",
    "    '''for split, dataset_sliced in sliced_ngram.items():\n",
    "        if split == 'train':\n",
    "            shuffle_ = True\n",
    "        else:\n",
    "            shuffle_ = False\n",
    "        dataset_ = NgramDataset(dataset_sliced)\n",
    "        ngram_datasets[split] = dataset_\n",
    "        ngram_loaders[split] = DataLoader(dataset_, batch_size=batch_size, shuffle=shuffle_, collate_fn=batchify)\n",
    "        '''\n",
    "    return ngram_datasets, ngram_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8K92PtdzerGJ",
    "outputId": "0a9d84b9-f7ae-42b0-cd05-211b45c6b474"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:00<00:00, 86934.05it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 122166.02it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 28968.26it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_datasets, wiki_loaders = build_ngram_loader(wiki_all, wiki_dict, 4, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64Ui351c1-Fk"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrIbhglGerGN"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "        logits = self.projection(lstm_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UqGD7dwerGP"
   },
   "outputs": [],
   "source": [
    "# check out eventually; below values are from lab 4\n",
    "# embedding_size = 256\n",
    "# hidden_size = 512\n",
    "# num_layers = 3\n",
    "# rnn_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "010w_DpGerGS"
   },
   "outputs": [],
   "source": [
    "### values included here are BASELINE only\n",
    "hidden_size = 128\n",
    "embedding_size = 64\n",
    "rnn_dropout = 0.1\n",
    "num_layers = 2\n",
    "\n",
    "options = {\n",
    "        'num_embeddings': len(wiki_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m16OksoXerGU"
   },
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = LSTMModel(options).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NTG0usGTj8AR",
    "outputId": "f9d5efb3-ac11-4cd2-bd7c-935ae5de9745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzItUEZBerGW"
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "lstm_criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
    "\n",
    "# lstm_model_parameters = [p for p in lstm_model.parameters() if p.requires_grad]\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Sw7FhqserGY"
   },
   "source": [
    "### Refactor for efficiency!\n",
    "in addition, may want to refactor similar to the way the lab implements this loop, where we allow for a pretrained model, i.e. we can provide a pretrained model to simply evaluate against validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_3wTEt1-of6"
   },
   "source": [
    "RENAME `lstm_perplexity` maybe _perplexity_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rY-VVNITq47s"
   },
   "outputs": [],
   "source": [
    "def lstm_perplexity(ce_loss):\n",
    "  return (2**(ce_loss/np.log(2)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKpFmj3serGZ"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def train_lstm(model, loaders_dict, criterion, optimizer, path_to_save = path+'baseline_lstm.pt'):\n",
    "    start_time = dt.datetime.now()\n",
    "    print('{} | Starting training'.format(start_time))\n",
    "    \n",
    "    \n",
    "    plot_cache = []\n",
    "    best_perplexity = float('Inf')\n",
    "    \n",
    "    for epoch_number in range(10):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch_number+1))\n",
    "        \n",
    "        avg_loss = -1.0\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        \n",
    "            #for i, (inp, target) in enumerate(persona_ngram_loaders['train']):\n",
    "        for i, (inp, target) in enumerate(loaders_dict['train']):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_log_cache.append(loss.item())  \n",
    "        \n",
    "            if i % 100 == 0: \n",
    "              print('{} | Number steps {}'.format(dt.datetime.now(), i))\n",
    "                \n",
    "        avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "        print('{} | Epoch {} avg train loss = {:.{prec}f}'.format(dt.datetime.now(), epoch_number+1, avg_loss, prec=4))\n",
    "     \n",
    "        train_log_cache = []\n",
    "      \n",
    "        #do valid\n",
    "        valid_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inp, target) in enumerate(loaders_dict['valid']):\n",
    "\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_losses.append(loss.item())\n",
    "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "            print('Epoch {} validation loss = {:.{prec}f}'.format(epoch_number+1, avg_val_loss, prec=4))\n",
    "\n",
    "            plot_cache.append((avg_loss, avg_val_loss))\n",
    "            val_ppl = lstm_perplexity(avg_val_loss)\n",
    "            print('Epoch {}, perplexity achieved {}'.format(epoch_number+1, val_ppl))\n",
    "            if val_ppl < best_perplexity:\n",
    "              torch.save(model.state_dict(), path_to_save)\n",
    "              best_perplexity = val_ppl\n",
    "    print('{} | Finished training in {}'.format(dt.datetime.now(), dt.datetime.now() - start_time))\n",
    "          \n",
    "    return plot_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhmLiUZ9erGb"
   },
   "outputs": [],
   "source": [
    "lstm_losses = train_lstm(lstm_model, wiki_loaders, lstm_criterion, lstm_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2jahfn5y79Fy",
    "outputId": "5bde4ff3-28ad-4ec1-9d20-90619bbc1396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, perplexity 485.74431725177425 \n",
      "Epoch 2, perplexity 340.5988893900643 \n",
      "Epoch 3, perplexity 282.0407894433271 \n",
      "Epoch 4, perplexity 248.94715152003695 \n",
      "Epoch 5, perplexity 227.81834640900593 \n",
      "Epoch 6, perplexity 211.63231959516264 \n",
      "Epoch 7, perplexity 202.8889041485416 \n",
      "Epoch 8, perplexity 195.41396969511615 \n",
      "Epoch 9, perplexity 189.1255822014498 \n",
      "Epoch 10, perplexity 185.00784711463538 \n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(lstm_losses):\n",
    "    print('Epoch {}, perplexity {} '.format(i+1,lstm_perplexity(j[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErCKgfq39ZVA"
   },
   "outputs": [],
   "source": [
    "pkl.dump(lstm_losses, open('baseline_lstm_losses.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QyimPD-17HP"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Mp-ARgg15ZN"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        rnn_outputs = self.rnn(embeddings)\n",
    "        logits = self.projection(rnn_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnrqhjEu7OCn"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "    path = 'gdrive/My Drive/'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "    path = './'\n",
    "\n",
    "rnn_model = RNNModel(options).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6IDaJ6O157U"
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "rnn_criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
    "\n",
    "# lstm_model_parameters = [p for p in lstm_model.parameters() if p.requires_grad]\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdSFddoQ2OWh"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def train_rnn(model, loaders_dict, criterion, optimizer, path_to_save = path+'baseline_rnn.pt'):\n",
    "    start_time = dt.datetime.now()\n",
    "    print('{} | Starting training'.format(start_time))\n",
    "    \n",
    "    \n",
    "    plot_cache = []\n",
    "    best_perplexity = float('Inf')\n",
    "    \n",
    "    for epoch_number in range(10):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch_number+1))\n",
    "        \n",
    "        avg_loss = -1.0\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        \n",
    "            #for i, (inp, target) in enumerate(persona_ngram_loaders['train']):\n",
    "        for i, (inp, target) in enumerate(loaders_dict['train']):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_log_cache.append(loss.item())  \n",
    "        \n",
    "            if i % 100 == 0: \n",
    "              print('{} | Number steps {}'.format(dt.datetime.now(), i))\n",
    "                \n",
    "        avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "        print('{} | Epoch {} avg train loss = {:.{prec}f}'.format(dt.datetime.now(), epoch_number+1, avg_loss, prec=4))\n",
    "     \n",
    "        train_log_cache = []\n",
    "      \n",
    "        #do valid\n",
    "        valid_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inp, target) in enumerate(loaders_dict['valid']):\n",
    "\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_losses.append(loss.item())\n",
    "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "            print('Epoch {} validation loss = {:.{prec}f}'.format(epoch_number+1, avg_val_loss, prec=4))\n",
    "\n",
    "            plot_cache.append((avg_loss, avg_val_loss))\n",
    "            \n",
    "            # Perplexity function is the same LSTM/RNN\n",
    "            val_ppl = lstm_perplexity(avg_val_loss)\n",
    "            print('Epoch {}, perplexity achieved {}'.format(epoch_number+1, val_ppl))\n",
    "            if val_ppl < best_perplexity:\n",
    "              torch.save(model.state_dict(), path_to_save)\n",
    "              best_perplexity = val_ppl\n",
    "    print('{} | Finished training in {}'.format(dt.datetime.now(), dt.datetime.now() - start_time))\n",
    "          \n",
    "    return plot_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1u5M1DNd2Og1",
    "outputId": "24eef1c9-0448-4a16-fe1d-4b476eb24b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-02 01:00:27.574590 | Starting training\n",
      "2019-10-02 01:00:27.576968 | Epoch 1\n",
      "2019-10-02 01:00:27.878605 | Number steps 0\n",
      "2019-10-02 01:00:47.922668 | Number steps 100\n",
      "2019-10-02 01:01:08.081708 | Number steps 200\n",
      "2019-10-02 01:01:28.989724 | Number steps 300\n",
      "2019-10-02 01:01:48.858173 | Number steps 400\n",
      "2019-10-02 01:02:08.982005 | Number steps 500\n",
      "2019-10-02 01:02:29.433522 | Number steps 600\n",
      "2019-10-02 01:02:31.459611 | Epoch 1 avg train loss = 6.4174\n",
      "Epoch 1 validation loss = 5.7755\n",
      "Epoch 1, perplexity achieved 322.2948576543607\n",
      "2019-10-02 01:02:37.076556 | Epoch 2\n",
      "2019-10-02 01:02:37.323707 | Number steps 0\n",
      "2019-10-02 01:02:57.181535 | Number steps 100\n",
      "2019-10-02 01:03:17.869424 | Number steps 200\n",
      "2019-10-02 01:03:38.507705 | Number steps 300\n",
      "2019-10-02 01:03:58.383793 | Number steps 400\n",
      "2019-10-02 01:04:18.900043 | Number steps 500\n",
      "2019-10-02 01:04:39.431193 | Number steps 600\n",
      "2019-10-02 01:04:41.610963 | Epoch 2 avg train loss = 5.8439\n",
      "Epoch 2 validation loss = 5.5393\n",
      "Epoch 2, perplexity achieved 254.4936345621147\n",
      "2019-10-02 01:04:47.122766 | Epoch 3\n",
      "2019-10-02 01:04:47.409798 | Number steps 0\n",
      "2019-10-02 01:05:07.536133 | Number steps 100\n",
      "2019-10-02 01:05:28.041639 | Number steps 200\n",
      "2019-10-02 01:05:48.563481 | Number steps 300\n",
      "2019-10-02 01:06:09.088268 | Number steps 400\n",
      "2019-10-02 01:06:29.200367 | Number steps 500\n",
      "2019-10-02 01:06:49.677177 | Number steps 600\n",
      "2019-10-02 01:06:51.782910 | Epoch 3 avg train loss = 5.6222\n",
      "Epoch 3 validation loss = 5.4184\n",
      "Epoch 3, perplexity achieved 225.51756898122392\n",
      "2019-10-02 01:06:57.341098 | Epoch 4\n",
      "2019-10-02 01:06:57.603796 | Number steps 0\n",
      "2019-10-02 01:07:18.308400 | Number steps 100\n",
      "2019-10-02 01:07:39.394379 | Number steps 200\n",
      "2019-10-02 01:07:59.097621 | Number steps 300\n",
      "2019-10-02 01:08:19.061231 | Number steps 400\n",
      "2019-10-02 01:08:39.515459 | Number steps 500\n",
      "2019-10-02 01:08:59.731878 | Number steps 600\n",
      "2019-10-02 01:09:01.727749 | Epoch 4 avg train loss = 5.4724\n",
      "Epoch 4 validation loss = 5.3474\n",
      "Epoch 4, perplexity achieved 210.06139386190586\n",
      "2019-10-02 01:09:07.263611 | Epoch 5\n",
      "2019-10-02 01:09:07.472265 | Number steps 0\n",
      "2019-10-02 01:09:27.859325 | Number steps 100\n",
      "2019-10-02 01:09:47.798896 | Number steps 200\n",
      "2019-10-02 01:10:07.867984 | Number steps 300\n",
      "2019-10-02 01:10:28.162717 | Number steps 400\n",
      "2019-10-02 01:10:49.485603 | Number steps 500\n",
      "2019-10-02 01:11:09.740574 | Number steps 600\n",
      "2019-10-02 01:11:11.833110 | Epoch 5 avg train loss = 5.3585\n",
      "Epoch 5 validation loss = 5.2987\n",
      "Epoch 5, perplexity achieved 200.07688413138428\n",
      "2019-10-02 01:11:17.393400 | Epoch 6\n",
      "2019-10-02 01:11:17.565598 | Number steps 0\n",
      "2019-10-02 01:11:38.104433 | Number steps 100\n",
      "2019-10-02 01:11:58.406852 | Number steps 200\n",
      "2019-10-02 01:12:18.899583 | Number steps 300\n",
      "2019-10-02 01:12:39.112153 | Number steps 400\n",
      "2019-10-02 01:12:59.572414 | Number steps 500\n",
      "2019-10-02 01:13:20.089233 | Number steps 600\n",
      "2019-10-02 01:13:22.147864 | Epoch 6 avg train loss = 5.2662\n",
      "Epoch 6 validation loss = 5.2702\n",
      "Epoch 6, perplexity achieved 194.4521044977905\n",
      "2019-10-02 01:13:27.625372 | Epoch 7\n",
      "2019-10-02 01:13:27.796006 | Number steps 0\n",
      "2019-10-02 01:13:48.867700 | Number steps 100\n",
      "2019-10-02 01:14:09.112506 | Number steps 200\n",
      "2019-10-02 01:14:28.576901 | Number steps 300\n",
      "2019-10-02 01:14:48.717098 | Number steps 400\n",
      "2019-10-02 01:15:09.314089 | Number steps 500\n",
      "2019-10-02 01:15:29.573553 | Number steps 600\n",
      "2019-10-02 01:15:31.710508 | Epoch 7 avg train loss = 5.1889\n",
      "Epoch 7 validation loss = 5.2455\n",
      "Epoch 7, perplexity achieved 189.71565241530683\n",
      "2019-10-02 01:15:37.416269 | Epoch 8\n",
      "2019-10-02 01:15:37.576210 | Number steps 0\n",
      "2019-10-02 01:15:57.654077 | Number steps 100\n",
      "2019-10-02 01:16:17.872074 | Number steps 200\n",
      "2019-10-02 01:16:37.945232 | Number steps 300\n",
      "2019-10-02 01:16:58.290428 | Number steps 400\n",
      "2019-10-02 01:17:19.188182 | Number steps 500\n",
      "2019-10-02 01:17:39.413268 | Number steps 600\n",
      "2019-10-02 01:17:41.621161 | Epoch 8 avg train loss = 5.1229\n",
      "Epoch 8 validation loss = 5.2270\n",
      "Epoch 8, perplexity achieved 186.2382209449899\n",
      "2019-10-02 01:17:47.151646 | Epoch 9\n",
      "2019-10-02 01:17:47.426057 | Number steps 0\n",
      "2019-10-02 01:18:07.151126 | Number steps 100\n",
      "2019-10-02 01:18:27.684233 | Number steps 200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b2e68e36d7e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline_rnn_losses.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-50f344a468d0>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(model, loaders_dict, criterion, optimizer, path_to_save)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 11.17 GiB total capacity; 5.20 GiB already allocated; 1.41 GiB free; 4.24 GiB cached)"
     ]
    }
   ],
   "source": [
    "rnn_losses = train_rnn(rnn_model, wiki_loaders, rnn_criterion, rnn_optimizer)\n",
    "pkl.dump(rnn_losses, open('baseline_rnn_losses.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq1aD3loD6jQ"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97gvAHpLerGd"
   },
   "source": [
    "### II.1 LSTM and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esNNWDIUerGd"
   },
   "outputs": [],
   "source": [
    "def grid_search(emb_dims, dropouts):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for emb_dim in emb_dims:\n",
    "        for dropout in dropouts:\n",
    "      \n",
    "            options = {\n",
    "                'num_embeddings': len(wiki_dict),\n",
    "                'embedding_dim': emb_dim,\n",
    "                'padding_idx': wiki_dict.get_id('<pad>'),\n",
    "                'input_size': emb_dim,\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'rnn_dropout': dropout,\n",
    "            }\n",
    "            lstm_model = LSTMModel(options).to(current_device)\n",
    "            lstm_losses = train_lstm(lstm_model, wiki_loaders, lstm_criterion, lstm_optimizer, path+ \"emb_dim=\" + str(emb_dim) + \"_dropout=\" + str(dropout) + \".pt\")\n",
    "            pkl.dump(lstm_losses, open(path+ \"emb_dim=\" + str(emb_dim) +  \"_dropout=\" + str(dropout) + \".p\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qL2QPU8terGf"
   },
   "source": [
    "#### Results (LSTM vs. Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9vX-91TerGg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xz5l25EperGh"
   },
   "source": [
    "#### Performance Variation Based on Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMlxEDtierGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01x_Q25GerGk"
   },
   "source": [
    "### II.2 Learned Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivqkICEkerGk"
   },
   "source": [
    "#### Utilities\n",
    "\n",
    "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
    "\n",
    "Use `!pip install umap-learn` to install UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqTr_62RerGl",
    "outputId": "31658b4d-2f00-4613-ea47-1cf5877cb79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def umap_plot(weight_matrix, word_ids, words):\n",
    "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
    "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
    "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    to_plot = reduced[word_ids, :]\n",
    "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        current_point = to_plot[i]\n",
    "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IWEkSzDerGn",
    "outputId": "53524f1e-e732-4b10-f17b-10c1eb8d9dae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vsize = 100                                 # e.g. len(dictionary)\n",
    "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
    "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
    "\n",
    "words = ['the', 'dog', 'ran']\n",
    "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
    "\n",
    "umap_plot(fake_weight_matrix, word_ids, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtBe0FmgerGo"
   },
   "source": [
    "#### II.2.1 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YuNafa2erGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJL5XRpDerGq"
   },
   "source": [
    "#### II.2.2 Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7sMAajEerGr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMSHRA6erGu"
   },
   "source": [
    "#### II.2.3 Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42ejnL7AerGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycvS0PEberGx"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxqEAO4IerGy"
   },
   "source": [
    "### II.3 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwoHDo4xerGy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58wLi3TIerG0"
   },
   "source": [
    "#### II.3.2 Highest and Lowest scoring sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S77x0WAOerG1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6lm2RPZerG3"
   },
   "source": [
    "#### II.3.3 Modified sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDlWT7pverG4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygWd_A72erG5"
   },
   "source": [
    "### II.4 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBqVQMHKerG6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID5eiL5JerG7"
   },
   "source": [
    "#### II.4.3 Number of unique tokens and sequence length \n",
    "\n",
    "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LL4wE3hFerG8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bMLc8_k6erG9"
   },
   "source": [
    "#### II.4.4 Example Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulJU-uFverG-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6QzTPpkRerFf",
    "R2n6VrfCerFi",
    "qL2QPU8terGf",
    "Xz5l25EperGh",
    "01x_Q25GerGk",
    "ivqkICEkerGk",
    "OtBe0FmgerGo",
    "OJL5XRpDerGq",
    "1WMSHRA6erGu",
    "WxqEAO4IerGy",
    "58wLi3TIerG0",
    "t6lm2RPZerG3",
    "ygWd_A72erG5",
    "ID5eiL5JerG7",
    "bMLc8_k6erG9"
   ],
   "name": "antonio_RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
